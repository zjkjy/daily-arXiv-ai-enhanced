<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: 本文研究了不同参数规模的SmolVLM2模型（500M和2.2B）在盲人和低视力用户视频描述任务中的性能表现，引入了两个专门针对BLV可访问性评估的新框架，并评估了不同提示策略和移动设备部署方案。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然能生成高质量视频描述，但其高内存、计算和部署需求限制了实际应用，特别是对于依赖详细、上下文感知描述的盲人和低视力用户。

Method: 使用500M和2.2B参数的SmolVLM2变体在AVCaps（户外）和Charades（室内）数据集上进行评估；引入多上下文BLV框架和导航辅助框架；系统评估四种提示设计策略；在智能手机上部署FP32和INT8精度变体。

Result: 评估了模型大小对可访问性描述质量的影响，以及不同精度模型在资源受限移动设备上的实际性能约束。

Conclusion: 研究为开发更适合BLV用户需求的轻量级视觉语言模型提供了重要见解，特别是在移动设备部署方面的可行性分析。

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [2] [One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models](https://arxiv.org/abs/2511.10629)
*Aleksandr Razin,Danil Kazantsev,Ilya Makarov*

Main category: cs.CV

TL;DR: 提出Latent Upscaler Adapter (LUA)，在潜在空间中直接进行超分辨率处理，避免扩散模型扩展到高分辨率时的缓慢采样和图像空间超分辨率带来的伪影和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型难以扩展到训练分辨率之外，直接高分辨率采样缓慢且昂贵，而图像空间超分辨率会在解码后引入伪影和额外延迟。

Method: LUA是一个轻量级模块，在最终VAE解码步骤之前直接在生成器的潜在代码上执行超分辨率。它作为即插即用组件集成，无需修改基础模型或添加额外扩散阶段，通过潜在空间中的单次前向传递实现高分辨率合成。

Result: LUA在解码和上采样时间上比像素空间SR快近3倍（1024px生成仅增加+0.42秒，而相同SwinIR架构的像素空间SR需要1.87秒），同时保持可比的感知质量。

Conclusion: LUA在保持原生高分辨率生成保真度的同时，为现代扩散管道中的可扩展、高保真图像合成提供了实用且高效的路径。

Abstract: Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.

</details>


### [3] [Depth Anything 3: Recovering the Visual Space from Any Views](https://arxiv.org/abs/2511.10647)
*Haotong Lin,Sili Chen,Junhao Liew,Donny Y. Chen,Zhenyu Li,Guang Shi,Jiashi Feng,Bingyi Kang*

Main category: cs.CV

TL;DR: Depth Anything 3 (DA3) 是一个从任意数量视觉输入预测空间一致几何的模型，无需已知相机位姿。通过最小化建模，使用单一普通Transformer作为骨干，单一深度射线预测目标，在教师-学生训练范式下达到与DA2相当的细节和泛化水平。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够从任意数量视觉输入预测空间一致几何的模型，无论是否已知相机位姿，追求最小化建模复杂度。

Method: 使用单一普通Transformer（如DINO编码器）作为骨干，无需架构专业化；采用单一深度射线预测目标，避免复杂多任务学习；通过教师-学生训练范式进行训练。

Result: 在新建的视觉几何基准测试中，DA3在所有任务上达到新的最先进水平，相机位姿精度平均超越先前SOTA VGGT 44.3%，几何精度提升25.1%；在单目深度估计上优于DA2。

Conclusion: DA3证明了使用简单架构和单一预测目标即可实现先进的几何预测性能，为视觉几何任务设定了新的基准，所有模型仅使用公共学术数据集训练。

Abstract: We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.

</details>


### [4] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: 本文提出自我一致性采样（SCS）方法，解决多模态大语言模型中结果奖励强化学习存在的缺陷——错误推理链但猜对选项也能获得相同奖励的问题。


<details>
  <summary>Details</summary>
Motivation: 在多模态推理基准测试中，结果奖励强化学习面临一个被忽视的障碍：即使推理链错误但猜对选项的轨迹也能获得与真正正确推理相同的奖励，这影响了模型的学习效果。

Method: 提出自我一致性采样（SCS）：对每个问题引入小的视觉扰动，并对初始轨迹进行重复截断和重采样；通过结果轨迹的一致性产生可微的一致性分数，在策略更新时降低不可靠轨迹的权重。

Result: 基于Qwen2.5-VL-7B-Instruct模型，将SCS集成到RLOO、GRPO和REINFORCE++系列中，在六个多模态基准测试上准确率提升高达7.7个百分点，且额外计算成本可忽略。在Qwen2.5-VL-3B-Instruct和InternVL3-8B模型上也取得显著提升。

Conclusion: SCS为多模态大语言模型中的结果奖励强化学习提供了一个简单通用的解决方案，能有效纠正不可靠推理轨迹的问题。

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [5] [Know Your Limits: Entropy Estimation Modeling for Compression and Generalization](https://arxiv.org/abs/2511.10618)
*Benjamin L. Badger,Matthew Neligeorge*

Main category: cs.CL

TL;DR: 本文引入编码器增强的因果解码器模型架构，在有限硬件条件下实现比因果变换器更高的压缩效率，并通过基于熵的训练方法提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于因果语言模型的语言压缩算法存在计算不可行性，无法准确估计语言熵，需要开发更高效的架构和训练方法。

Method: 提出编码器增强的因果解码器模型架构，通过基于每词元熵估计的训练方法，使模型接近但不超出训练数据的熵值。

Result: 新架构在训练效率上优于因果变换器，实现更高压缩比；基于熵的训练方法显著提升了模型泛化能力。

Conclusion: 接近但不超出语言熵的训练策略是提升语言模型泛化能力的关键，编码器增强架构为高效语言压缩提供了可行方案。

Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.

</details>


### [6] [SSR: Socratic Self-Refine for Large Language Model Reasoning](https://arxiv.org/abs/2511.10621)
*Haizhou Shi,Ye Liu,Bo Pang,Zeyu Leo Liu,Hao Wang,Silvio Savarese,Caiming Xiong,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: 提出Socratic Self-Refine (SSR)框架，通过细粒度评估和精确精炼来改进大语言模型的推理能力，将模型响应分解为可验证的子问题-子答案对，实现步骤级置信度估计和迭代精炼。


<details>
  <summary>Details</summary>
Motivation: 现有测试时框架依赖粗粒度的自我验证和自我修正，在复杂任务上效果有限，需要更精细的方法来评估和改进LLM的推理过程。

Method: SSR框架将模型响应分解为可验证的(子问题,子答案)对，通过受控重解和自一致性检查进行步骤级置信度估计，识别不可靠步骤并迭代精炼。

Result: 在五个推理基准和三个LLM上的实证结果显示，SSR始终优于最先进的迭代自我精炼基线方法。

Conclusion: SSR不仅提升了性能，还提供了一种原则性的黑盒方法来评估和理解LLM的内部推理过程。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.

</details>


### [7] [Instella: Fully Open Language Models with Stellar Performance](https://arxiv.org/abs/2511.10628)
*Jiang Liu,Jialian Wu,Xiaodong Yu,Yusheng Su,Prakamya Mishra,Gowtham Ramesh,Sudhanshu Ranjan,Chaitanya Manem,Ximeng Sun,Ze Wang,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: Instella是一个完全开源的30亿参数语言模型家族，使用公开数据训练，在AMD MI300X GPU上开发，包含通用版本和两个专业变体：支持128K上下文长度的Instella-Long和数学推理优化的Instella-Math。


<details>
  <summary>Details</summary>
Motivation: 当前高性能语言模型多为闭源或部分开源，限制了透明度和可复现性。本研究旨在开发完全开源的替代方案，推动开放和可复现的语言建模研究。

Method: 采用大规模预训练、通用指令微调和人类偏好对齐的方法，使用AMD Instinct MI300X GPU进行训练。开发了两个专业变体：通过监督微调和强化学习优化的数学推理模型，以及支持长上下文的模型。

Result: 尽管使用的预训练token数量明显少于同类模型，Instella在完全开源模型中达到最先进水平，并与同规模领先开源权重模型具有竞争力。

Conclusion: Instella系列模型为社区提供了透明、高性能且多功能的替代方案，推进了开放和可复现语言建模研究的目标。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.

</details>


### [8] [ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference](https://arxiv.org/abs/2511.10645)
*Yesheng Liang,Haisheng Chen,Song Han,Zhijian Liu*

Main category: cs.CL

TL;DR: ParoQuant是一种权重后训练量化方法，通过成对旋转量化和通道级缩放来减少异常值影响，在推理任务上比AWQ平均提升2.4%准确率，同时保持小于10%的开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型权重后训练量化中存在权重和激活值的异常值问题，导致量化误差大和精度下降，特别是在推理模型中误差会在长思维链中累积。现有方法要么无法充分抑制异常值，要么引入显著推理开销。

Method: 提出Pairwise Rotation Quantization (ParoQuant)，结合硬件高效的独立Givens旋转和通道级缩放，均衡通道间幅度并缩小每个量化组内的动态范围。同时协同设计推理内核以充分利用GPU并行性，保持旋转和缩放操作的轻量级运行。

Result: 在推理任务上比AWQ平均提升2.4%准确率，同时保持小于10%的开销。

Conclusion: 该方法为推理型大型语言模型提供了更高效和准确的部署途径。

Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Querying Labeled Time Series Data with Scenario Programs](https://arxiv.org/abs/2511.10627)
*Edward Kim,Devan Shanker,Varun Bharadwaj,Hongbeen Park,Jinkyu Kim,Hazem Torfah,Daniel J Fremont,Sanjit A Seshia*

Main category: cs.AI

TL;DR: 本文提出了一种验证模拟故障场景在真实世界中可复现性的方法，通过定义时间序列传感器数据与抽象场景的匹配关系，开发了高效的查询算法来在真实数据集中定位模拟发现的故障场景。


<details>
  <summary>Details</summary>
Motivation: 解决仿真到现实的差距问题，验证在仿真环境中发现的自动驾驶系统故障场景是否能在真实世界中复现，避免仿真场景成为合成传感器数据的假象。

Method: 引入形式化定义来描述标记时间序列传感器数据与Scenic概率编程语言表示的抽象场景的匹配关系，提出查询算法在给定数据集和场景程序时识别匹配的数据子集。

Result: 实验表明，该算法比最先进的商业视觉大语言模型更准确，查询速度提高了数个数量级，且能随着查询时间序列数据时长而扩展。

Conclusion: 该方法有效解决了仿真到现实的验证问题，为自动驾驶系统安全测试提供了可靠的仿真故障场景验证手段。

Abstract: Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem](https://arxiv.org/abs/2511.10619)
*Avrim Blum,Marten Garicano,Kavya Ravichandran,Dravyansh Sharma*

Main category: cs.LG

TL;DR: 本文提出了两种参数化的多臂赌博机算法族，通过离线数据学习近最优算法，在奖励曲线满足特定凹性条件时获得更强的性能保证，同时保证在良好实例上实现最优臂识别，在恶劣实例上回退到最坏情况保证。


<details>
  <summary>Details</summary>
Motivation: 改进型多臂赌博机问题用于在不确定性下分配资源，如技术研发投资、临床试验和超参数选择。现有算法的最坏情况保证较为悲观，存在较强的下界限制。本文旨在通过参数化算法族和数据依赖分析来突破这些限制。

Method: 提出了两种参数化算法族：第一种包含先前工作中的最优随机算法，在奖励曲线满足额外凹性性质时可获得更强的性能保证；第二种算法族保证在良好实例上实现最优臂识别，在恶劣实例上回退到最坏情况保证。采用统计学习视角，通过离线数据学习近最优算法。

Result: 当臂奖励曲线满足与凹性强弱相关的额外性质时，从第一个算法族中选择的适当算法可以实现更强的保证，其性能对臂数k具有最优依赖性。第二个算法族提供了数据依赖的保证，无需验证假设是否满足。

Conclusion: 通过参数化算法族和统计学习视角，本文在改进型多臂赌博机问题上实现了更强的数据依赖性能保证，突破了传统最坏情况分析的限制，为实际应用提供了更实用的算法框架。

Abstract: The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $Ω(k)$ and $Ω(\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.

</details>
